HyperTalk mobile app and the website both are computer vision-based solutions to facilitate communication in sign language for individuals with hearing and talking issues. Which is capable of real-time translation in both directions: 
        1] Sign language camera feed to Voice. 
        2] Voice feed to sign language animations.
    
We are currently improving the 1st feature with a new Isolated Word-level Sign Language Recognition model which is capable of more accurate and faster translations with different sign language options for different regions in the world - Data sets: \\
        1] Phoenix 2014 Dataset (German Sign Language Videos)  \\ 
        2] OpenASL Dataset (American Sign Language Videos) \\
        3] CSL Dataset  \\
        4] BOBSL Dataset (British Sign Language Videos)
        Tools \& technologies used: PyTorch, CUDA, OpenCV, Flutter UI, Django back-end development
